Udacity's CS101 - Buildind a Search Engine Contest

At the end of Udacity's (www.udacity.com) first instance of
the CS101 course (ending in April 2012) the course staff
issued a contest for all students to build on the ideas in the 
CS101 class in a creative way. This is my entry to the contest.


* Preamble

During the seven weeks of the CS101 course, we built a basic 
seach engine consisting of a web crawler, a lookup functionality
and a ranking algorithm. The language used was Python 2.x.


* Background

One of the more challenging parts of the course was the 
implementation of a ranking algorithm similar to PageRank. In this
approach the ranking is determined from the linked structure of
web document, that is, the rank is completely determined from a
graph representing the web. This graph has as nodes the pages and
as edges the links between pages.


* The Idea

My contribution to this contest is a very simple set of functions 
that take the graph representing the web and outpus a file in the
DOT language (https://en.wikipedia.org/wiki/DOT_language). This file
can be used by, for example, Graphviz (www.graphviz.org) to produce 
an ilustrative image of the graph thus making easier to understand
the results of the ranking algorithm.


* Implementation

The code consists of only two files

- search_engine.py : This file contains the final code from the course.
Here is the web crawler, the ranking and lookup procedures. This file
doesn't contain any original contribution by me.
- graphviz.py : Here I define the procedures that take the results from
the search engine and produce strings in appropiate format to be writen
to a DOT file. Each node has its (abbreviated) url and its rank as the
displayed name. Nodes vary in sizes according to their rank.

To produce an actual image from the DOT file you will need to download
the Graphviz package (www.graphviz.org) or something similar.


* Usage

Start a Python interpreted in the folder in which you have the code. Then
import the graphviz.py functionalities.

#+BEGIN_SRC python
  from graphviz import *
#+END_SRC

We have now loaded the toy-web example from the course and we have some
variables defined: index, graph and ranks. With this we can produce a dot
file of the complete web

#+BEGIN_SRC python
  write_dot_file("web.dot", graph, ranks)
#+END_SRC

This will create the file web.dot which can be used by Graphviz to produce,
for example, an svg image. To do that, from the console type

#+BEGIN_SRC shell
dot -Tsvg web.dot -o web.svg
#+END_SRC

You can now view the web.svg image, for example, in your web browser. There 
is also another funtion that makes a lookup and produces a DOT file with a
central node, which is the highest ranking result of the lookup, all the 
nodes linking to and from this central node. For example, a lookup for
"Kidnap"

#+BEGIN_SRC python
  write_dot_lookup("Kidnap_lookup.dot", index, ranks, "Kidnap", graph)
#+END_SRC

and again, to produce the actual image, type from a console

#+BEGIN_SRC shell
dot -Tsvg Kidnap_lookup.dot -o Kidnap_lookup.svg
#+END_SRC

You could also try to crawl something different from the provided examples. The
get_page procedure will try to fetch pages from the web, however this could 
take a long time. There is a global variable crawl_depth (defaulted to 1) that
specifies the number of links to be followed from the seed page. To crawl something
else and make a query there

#+BEGIN_SRC python
  index2, graph2 = crawl_web("http://www.xkcd.com/")
  ranks2 = compute_ranks(graph2)
  write_dot_lookup("student.dot", index2, ranks2, "student", graph2)
#+END_SRC

and use Graphviz as before to produce an image. However, due to several limitation
in the crawling, the string splitting and glitches with my graphviz procedures the
images produced from "real world" pages are still terrible.


* To Do

I see several things that can be improved in all the areas and I've been playing
around with this code only for a couple of days, so surely many more will arise. One 
of the most obvious problem is when trying to make an image of a "big" web; there 
are pages with many outgoing links that clutter the whole place. Also the string
splitting and the crawling could be improved a lot.

At the top of my head I can think of these improvements:
- [ ] When there are a lot of nodes to be used in an image, remove the smaller ones to reduce cluttering.
- [ ] Better split the strings to avoid including html tags in the index.
- [ ] An easier way to produce the images. There is too much interaction with the interpreter.
- [ ] When making a lookup, make the node with the keyword have a different color.
- [ ] It would be nice to be able to produce a video of the evolution of the ranks as we iterate through the ranking algorithm
- [ ] When one page has more than one link to another page, instead of having many arrows it would be better to have one single arrow but with a label indicating the number of links.
